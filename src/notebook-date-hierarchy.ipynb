{"cells":[{"cell_type":"markdown","source":["# Fabric Date Table Generator (PySpark Approach)\n","\n","This notebook generates a dynamic date table for use in Microsoft Fabric Lakehouse and Power BI Direct Lake. It is structured for clarity, flexibility, and maintainability."],"metadata":{"language":"markdown"},"id":"d1efe5ff-5486-488f-b190-a7c3f9181c69"},{"cell_type":"markdown","source":["## Parameters\n","\n","All parameters are defined here for easy customization. Adjust these as needed for your scenario.\n","\n","- **start_date**: First date in the table (YYYY-MM-DD)\n","- **end_date**: Last date in the table (YYYY-MM-DD)\n","- **fiscal_year_start_month**: Month (1-12) when the fiscal year starts\n","- **holiday_dates**: List of holiday dates (YYYY-MM-DD) to flag in the table"],"metadata":{"language":"markdown"},"id":"d32234da-cdbe-4c8a-ba80-be6e15155941"},{"cell_type":"code","source":["# Parameters\n","from datetime import date\n","\n","start_date = date(2015, 1, 1)\n","end_date = date(2030, 12, 31)\n","fiscal_year_start_month = 7  # July\n","holiday_dates = [\n","    date(2025, 1, 1),\n","    date(2025, 12, 25)\n","    ]"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"ad2d7c8f-795e-4055-8a1c-b5326240b671","normalized_state":"finished","queued_time":"2025-07-08T19:18:26.0784424Z","session_start_time":"2025-07-08T19:18:26.0811637Z","execution_start_time":"2025-07-08T19:18:37.3875638Z","execution_finish_time":"2025-07-08T19:18:37.7050602Z","parent_msg_id":"a04ceb12-2f46-49ba-807d-cca8dc534886"},"text/plain":"StatementMeta(, ad2d7c8f-795e-4055-8a1c-b5326240b671, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"74bda010-2bd7-465c-a315-3d6eb9575ef0"},{"cell_type":"markdown","source":["## Generate Date Range\n","\n","We use PySpark to generate the date range directly for scalability. This approach is efficient for large ranges and avoids pandas bottlenecks."],"metadata":{"language":"markdown"},"id":"49624daf-0b39-438e-a038-2e5401de669c"},{"cell_type":"code","source":["from pyspark.sql.functions import sequence, to_date, col, explode, expr\n","from pyspark.sql.types import DateType\n","\n","# Create a Spark DataFrame with start and end dates\n","spark_df = spark.createDataFrame([(start_date, end_date)], [\"start\", \"end\"])\n","\n","# Generate date range using sequence with INTERVAL\n","date_sdf = spark_df.select(\n","    explode(expr(\"sequence(start, end, interval 1 day)\")).alias(\"Date\")\n",").withColumn(\"Date\", to_date(col(\"Date\")))\n","\n","# Show results\n","print(f\"Generated {date_sdf.count()} rows for date range.\")\n","date_sdf.show(5)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"ad2d7c8f-795e-4055-8a1c-b5326240b671","normalized_state":"finished","queued_time":"2025-07-08T19:18:26.0825927Z","session_start_time":null,"execution_start_time":"2025-07-08T19:18:37.7072708Z","execution_finish_time":"2025-07-08T19:18:41.4256433Z","parent_msg_id":"065aa3a3-1c7d-4e7e-943c-e4f3a77bb017"},"text/plain":"StatementMeta(, ad2d7c8f-795e-4055-8a1c-b5326240b671, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Generated 5844 rows for date range.\n+----------+\n|      Date|\n+----------+\n|2015-01-01|\n|2015-01-02|\n|2015-01-03|\n|2015-01-04|\n|2015-01-05|\n+----------+\nonly showing top 5 rows\n\n"]}],"execution_count":2,"metadata":{"language":"python","microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b3fa64be-b9e2-4459-a449-31db4e15f347"},{"cell_type":"markdown","source":["## Add Date Columns\n","\n","We add calendar, fiscal, and time intelligence columns. All logic is vectorized for performance."],"metadata":{"language":"markdown"},"id":"99c278c8-a873-4008-94d3-f82fe786a055"},{"cell_type":"code","source":["from pyspark.sql.functions import year, month, dayofmonth, dayofweek, weekofyear, quarter, date_format, when, lit\n","\n","df = date_sdf \\\n","    .withColumn(\"Year\", year(col(\"Date\"))) \\\n","    .withColumn(\"Month\", month(col(\"Date\"))) \\\n","    .withColumn(\"Day\", dayofmonth(col(\"Date\"))) \\\n","    .withColumn(\"Quarter\", quarter(col(\"Date\"))) \\\n","    .withColumn(\"MonthName\", date_format(col(\"Date\"), \"MMMM\")) \\\n","    .withColumn(\"DayOfWeek\", dayofweek(col(\"Date\"))) \\\n","    .withColumn(\"WeekOfYear\", weekofyear(col(\"Date\")))\n","\n","# Fiscal Year/Quarter/Month logic\n","df = df \\\n","    .withColumn(\"FiscalYear\",\n","        when(col(\"Month\") >= fiscal_year_start_month, col(\"Year\") + 1).otherwise(col(\"Year\"))\n","    ) \\\n","    .withColumn(\"FiscalMonth\", ((col(\"Month\") - fiscal_year_start_month + 12) % 12 + 1)) \\\n","    .withColumn(\"FiscalQuarter\", ((col(\"FiscalMonth\") - 1) / 3 + 1).cast(\"int\"))\n","\n","# Holiday flag\n","from pyspark.sql.functions import array, lit as spark_lit\n","df = df.withColumn(\"IsHoliday\", col(\"Date\").cast(\"string\").isin([d.isoformat() for d in holiday_dates]))\n","\n","# Current day/month/year flags\n","from datetime import datetime\n","today = datetime.today().date()\n","df = df \\\n","    .withColumn(\"IsCurrentDay\", col(\"Date\") == spark_lit(today.isoformat())) \\\n","    .withColumn(\"IsCurrentMonth\", (col(\"Year\") == today.year) & (col(\"Month\") == today.month)) \\\n","    .withColumn(\"IsCurrentYear\", col(\"Year\") == today.year)\n","\n","df.show(5)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"ad2d7c8f-795e-4055-8a1c-b5326240b671","normalized_state":"finished","queued_time":"2025-07-08T19:18:26.0892071Z","session_start_time":null,"execution_start_time":"2025-07-08T19:18:41.4277976Z","execution_finish_time":"2025-07-08T19:18:42.9318169Z","parent_msg_id":"5897fcfb-049a-4209-a467-8a33ad98d9ce"},"text/plain":"StatementMeta(, ad2d7c8f-795e-4055-8a1c-b5326240b671, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+----------+----+-----+---+-------+---------+---------+----------+----------+-----------+-------------+---------+------------+--------------+-------------+\n|      Date|Year|Month|Day|Quarter|MonthName|DayOfWeek|WeekOfYear|FiscalYear|FiscalMonth|FiscalQuarter|IsHoliday|IsCurrentDay|IsCurrentMonth|IsCurrentYear|\n+----------+----+-----+---+-------+---------+---------+----------+----------+-----------+-------------+---------+------------+--------------+-------------+\n|2015-01-01|2015|    1|  1|      1|  January|        5|         1|      2015|          7|            3|    false|       false|         false|        false|\n|2015-01-02|2015|    1|  2|      1|  January|        6|         1|      2015|          7|            3|    false|       false|         false|        false|\n|2015-01-03|2015|    1|  3|      1|  January|        7|         1|      2015|          7|            3|    false|       false|         false|        false|\n|2015-01-04|2015|    1|  4|      1|  January|        1|         1|      2015|          7|            3|    false|       false|         false|        false|\n|2015-01-05|2015|    1|  5|      1|  January|        2|         2|      2015|          7|            3|    false|       false|         false|        false|\n+----------+----+-----+---+-------+---------+---------+----------+----------+-----------+-------------+---------+------------+--------------+-------------+\nonly showing top 5 rows\n\n"]}],"execution_count":3,"metadata":{"language":"python","microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e61eb958-f1d5-4083-8737-86ddf0837059"},{"cell_type":"markdown","source":["## Write to Lakehouse Table\n","\n","We save the date table to the Lakehouse in Delta format. This enables Direct Lake access in Power BI for real-time analytics.\n","\n","- Table name: `date_table`\n","- Mode: `overwrite` (regenerates the table each run)"],"metadata":{"language":"markdown"},"id":"fad03239-bc07-4c92-b0e6-335a60352869"},{"cell_type":"code","source":["df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"date_table\")\n","print(\"Date table written to Lakehouse as 'date_table'.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"ad2d7c8f-795e-4055-8a1c-b5326240b671","normalized_state":"finished","queued_time":"2025-07-08T19:18:26.0915277Z","session_start_time":null,"execution_start_time":"2025-07-08T19:18:42.9339351Z","execution_finish_time":"2025-07-08T19:18:56.8087241Z","parent_msg_id":"ae56dc88-193e-461a-ac86-563599e8130c"},"text/plain":"StatementMeta(, ad2d7c8f-795e-4055-8a1c-b5326240b671, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Date table written to Lakehouse as 'date_table'.\n"]}],"execution_count":4,"metadata":{"language":"python","microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"69fe5efa-f874-4a56-9369-fd1425df6935"},{"cell_type":"markdown","source":["## Testing & Validation\n","\n","We include basic tests to validate fiscal year logic and print sample output for review."],"metadata":{"language":"markdown"},"id":"bef31c42-801e-4b26-bcd5-6ce849a9e414"},{"cell_type":"code","source":["# Test: Fiscal year boundary\n","import pandas as pd\n","test_date = pd.Timestamp(year=2025, month=fiscal_year_start_month, day=1)\n","fy = test_date.year + (1 if test_date.month >= fiscal_year_start_month else 0)\n","row = df.filter(df.Date == test_date.isoformat()).select(\"FiscalYear\").collect()\n","if row:\n","    assert row[0][0] == fy, f\"FiscalYear logic failed for {test_date}\"\n","print(\"FiscalYear logic test passed.\")\n","\n","# Show first and last few rows\n","df.orderBy(\"Date\").show(3)\n","df.orderBy(col(\"Date\").desc()).show(3)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"ad2d7c8f-795e-4055-8a1c-b5326240b671","normalized_state":"finished","queued_time":"2025-07-08T19:18:26.1308292Z","session_start_time":null,"execution_start_time":"2025-07-08T19:18:56.8108843Z","execution_finish_time":"2025-07-08T19:18:59.2764005Z","parent_msg_id":"4f530aef-fd1a-46bc-a180-a1f9c43e2d35"},"text/plain":"StatementMeta(, ad2d7c8f-795e-4055-8a1c-b5326240b671, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["FiscalYear logic test passed.\n+----------+----+-----+---+-------+---------+---------+----------+----------+-----------+-------------+---------+------------+--------------+-------------+\n|      Date|Year|Month|Day|Quarter|MonthName|DayOfWeek|WeekOfYear|FiscalYear|FiscalMonth|FiscalQuarter|IsHoliday|IsCurrentDay|IsCurrentMonth|IsCurrentYear|\n+----------+----+-----+---+-------+---------+---------+----------+----------+-----------+-------------+---------+------------+--------------+-------------+\n|2015-01-01|2015|    1|  1|      1|  January|        5|         1|      2015|          7|            3|    false|       false|         false|        false|\n|2015-01-02|2015|    1|  2|      1|  January|        6|         1|      2015|          7|            3|    false|       false|         false|        false|\n|2015-01-03|2015|    1|  3|      1|  January|        7|         1|      2015|          7|            3|    false|       false|         false|        false|\n+----------+----+-----+---+-------+---------+---------+----------+----------+-----------+-------------+---------+------------+--------------+-------------+\nonly showing top 3 rows\n\n+----------+----+-----+---+-------+---------+---------+----------+----------+-----------+-------------+---------+------------+--------------+-------------+\n|      Date|Year|Month|Day|Quarter|MonthName|DayOfWeek|WeekOfYear|FiscalYear|FiscalMonth|FiscalQuarter|IsHoliday|IsCurrentDay|IsCurrentMonth|IsCurrentYear|\n+----------+----+-----+---+-------+---------+---------+----------+----------+-----------+-------------+---------+------------+--------------+-------------+\n|2030-12-31|2030|   12| 31|      4| December|        3|         1|      2031|          6|            2|    false|       false|         false|        false|\n|2030-12-30|2030|   12| 30|      4| December|        2|         1|      2031|          6|            2|    false|       false|         false|        false|\n|2030-12-29|2030|   12| 29|      4| December|        1|        52|      2031|          6|            2|    false|       false|         false|        false|\n+----------+----+-----+---+-------+---------+---------+----------+----------+-----------+-------------+---------+------------+--------------+-------------+\nonly showing top 3 rows\n\n"]}],"execution_count":5,"metadata":{"language":"python","microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8e7c48f8-b0e4-43d6-ac3a-b1384e8d9f92"},{"cell_type":"markdown","source":["## Notes & Best Practices\n","\n","- All calculations are performed upstream for Direct Lake compatibility.\n","- The notebook is parameterized for easy reuse.\n","- For very large date ranges, Spark generation is scalable.\n","- The output table can be shared across Power BI datasets for consistency.\n","- For maintenance, update the end date and holiday list as needed."],"metadata":{"language":"markdown"},"id":"319b0cbe-c16d-4e3e-84c2-c1ed9fd4781d"}],"metadata":{"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"kernel_info":{"name":"synapse_pyspark"},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"ed0e13b7-a430-4ea7-a246-3483c417e6e1"}],"default_lakehouse":"ed0e13b7-a430-4ea7-a246-3483c417e6e1","default_lakehouse_name":"configlakehouse","default_lakehouse_workspace_id":"890c1e73-ad9b-4b53-86d9-f3308624208a"}}},"nbformat":4,"nbformat_minor":5}